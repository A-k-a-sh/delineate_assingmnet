{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66ce085e",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26712eda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:06:27.267016Z",
     "iopub.status.busy": "2026-01-30T06:06:27.266419Z",
     "iopub.status.idle": "2026-01-30T06:06:34.028663Z",
     "shell.execute_reply": "2026-01-30T06:06:34.028006Z",
     "shell.execute_reply.started": "2026-01-30T06:06:27.266985Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.8.0+cu126\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0113a3b",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4080fc3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:06:34.030521Z",
     "iopub.status.busy": "2026-01-30T06:06:34.030128Z",
     "iopub.status.idle": "2026-01-30T06:06:34.038904Z",
     "shell.execute_reply": "2026-01-30T06:06:34.038202Z",
     "shell.execute_reply.started": "2026-01-30T06:06:34.030497Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Crop size: 128×384\n",
      "  Backbone: resnet18\n",
      "  Batch size: 32\n",
      "  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Data paths\n",
    "    synthetic_images: str = \"/kaggle/input/med-synthetic-dataset/kaggle/working/dataset/images\"\n",
    "    synthetic_labels: str = \"/kaggle/input/med-synthetic-dataset/kaggle/working/dataset/labels\"\n",
    "    \n",
    "    real_images: str = \"/kaggle/input/med-real-dataset/dataset/images\"\n",
    "    real_labels: str = \"/kaggle/input/med-real-dataset/dataset/labels\"\n",
    "    \n",
    "    # Crop parameters\n",
    "    crop_width: int = 128      # Width of crop around data point\n",
    "    crop_height: int = 384     # Height of crop (more vertical space for error bars)\n",
    "    max_offset: float = 280.0  # Maximum expected error bar length (for normalization)\n",
    "    \n",
    "    # Model parameters\n",
    "    backbone: str = \"resnet18\"  # resnet18, resnet34\n",
    "    pretrained: bool = True\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size: int = 32\n",
    "    num_epochs_pretrain: int = 6\n",
    "    num_epochs_finetune: int = 10\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    \n",
    "    # Data split\n",
    "    train_split: float = 0.95   # 95% of synthetic for training\n",
    "    real_train_split: float = 0.8  # 80% of real for fine-tuning\n",
    "    \n",
    "    # Output\n",
    "    checkpoint_dir: str = \"checkpoints\"\n",
    "    results_dir: str = \"results\"\n",
    "\n",
    "cfg = Config()\n",
    "os.makedirs(cfg.checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(cfg.results_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Crop size: {cfg.crop_width}×{cfg.crop_height}\")\n",
    "print(f\"  Backbone: {cfg.backbone}\")\n",
    "print(f\"  Batch size: {cfg.batch_size}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd087a86",
   "metadata": {},
   "source": [
    "## 3. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d389ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:06:34.040482Z",
     "iopub.status.busy": "2026-01-30T06:06:34.040122Z",
     "iopub.status.idle": "2026-01-30T06:06:34.058548Z",
     "shell.execute_reply": "2026-01-30T06:06:34.057949Z",
     "shell.execute_reply.started": "2026-01-30T06:06:34.040460Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class defined\n"
     ]
    }
   ],
   "source": [
    "class ErrorBarDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for error bar detection.\n",
    "    \n",
    "    Returns crops around each data point with ground truth offsets.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        image_ids: List[str],\n",
    "        images_dir: str,\n",
    "        labels_dir: str,\n",
    "        crop_width: int = 128,\n",
    "        crop_height: int = 384,\n",
    "        max_offset: float = 280.0,\n",
    "        transform=None,\n",
    "        augment: bool = False\n",
    "    ):\n",
    "        self.image_ids = image_ids\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.crop_width = crop_width\n",
    "        self.crop_height = crop_height\n",
    "        self.max_offset = max_offset\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Build index of all data points\n",
    "        self.samples = []\n",
    "        self._build_index()\n",
    "        \n",
    "        print(f\"Dataset: {len(self.samples)} data points from {len(image_ids)} images\")\n",
    "    \n",
    "    def _build_index(self):\n",
    "        \"\"\"Build index of all data points across all images.\"\"\"\n",
    "        for img_id in self.image_ids:\n",
    "            label_path = os.path.join(self.labels_dir, f\"{img_id}.json\")\n",
    "            if not os.path.exists(label_path):\n",
    "                continue\n",
    "            \n",
    "            with open(label_path, 'r') as f:\n",
    "                label = json.load(f)\n",
    "            \n",
    "            # Extract all actual data points (not anchor points)\n",
    "            for line in label:\n",
    "                for point in line[\"points\"]:\n",
    "                    if point.get(\"label\", \"\") == \"\":  # actual data point\n",
    "                        self.samples.append({\n",
    "                            \"image_id\": img_id,\n",
    "                            \"x\": float(point[\"x\"]),\n",
    "                            \"y\": float(point[\"y\"]),\n",
    "                            \"dy_up\": float(point.get(\"topBarPixelDistance\", 0)),\n",
    "                            \"dy_down\": float(point.get(\"bottomBarPixelDistance\", 0))\n",
    "                        })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(self.images_dir, f\"{sample['image_id']}.png\")\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_w, img_h = img.size\n",
    "        \n",
    "        # Extract crop around data point\n",
    "        x, y = int(sample['x']), int(sample['y'])\n",
    "        \n",
    "        # Crop boundaries (centered on data point)\n",
    "        x1 = max(0, x - self.crop_width // 2)\n",
    "        x2 = min(img_w, x + self.crop_width // 2)\n",
    "        y1 = max(0, y - self.crop_height // 2)\n",
    "        y2 = min(img_h, y + self.crop_height // 2)\n",
    "        \n",
    "        crop = img.crop((x1, y1, x2, y2))\n",
    "        \n",
    "        # Pad if crop is too small (near edges)\n",
    "        if crop.size != (self.crop_width, self.crop_height):\n",
    "            padded = Image.new('RGB', (self.crop_width, self.crop_height), (255, 255, 255))\n",
    "            paste_x = (self.crop_width - crop.size[0]) // 2\n",
    "            paste_y = (self.crop_height - crop.size[1]) // 2\n",
    "            padded.paste(crop, (paste_x, paste_y))\n",
    "            crop = padded\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            crop = self.transform(crop)\n",
    "        \n",
    "        # Ground truth offsets (normalized)\n",
    "        dy_up = sample['dy_up'] / self.max_offset\n",
    "        dy_down = sample['dy_down'] / self.max_offset\n",
    "        target = torch.tensor([dy_up, dy_down], dtype=torch.float32)\n",
    "        \n",
    "        return crop, target\n",
    "\n",
    "print(\"Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135d1899",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5220812",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:06:34.059517Z",
     "iopub.status.busy": "2026-01-30T06:06:34.059324Z",
     "iopub.status.idle": "2026-01-30T06:06:35.775379Z",
     "shell.execute_reply": "2026-01-30T06:06:35.774620Z",
     "shell.execute_reply.started": "2026-01-30T06:06:34.059490Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 160MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: resnet18\n",
      "Total parameters: 11,324,418\n",
      "Trainable parameters: 11,324,418\n",
      "Output shape: torch.Size([2, 2])  # (batch_size, 2)\n"
     ]
    }
   ],
   "source": [
    "class ErrorBarRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN-based regressor for error bar endpoint detection.\n",
    "    \n",
    "    Architecture:\n",
    "    - ResNet backbone (pretrained on ImageNet)\n",
    "    - Global average pooling\n",
    "    - FC layers → 2 outputs (dy_up, dy_down)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, backbone='resnet18', pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained backbone\n",
    "        if backbone == 'resnet18':\n",
    "            resnet = models.resnet18(pretrained=pretrained)\n",
    "            feat_dim = 512\n",
    "        elif backbone == 'resnet34':\n",
    "            resnet = models.resnet34(pretrained=pretrained)\n",
    "            feat_dim = 512\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown backbone: {backbone}\")\n",
    "        \n",
    "        # Remove final FC layer\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        \n",
    "        # Regression head\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 2)  # (dy_up, dy_down)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.backbone(x)  # (batch, feat_dim, 1, 1)\n",
    "        features = features.view(features.size(0), -1)  # (batch, feat_dim)\n",
    "        \n",
    "        # Regress offsets\n",
    "        offsets = self.regressor(features)  # (batch, 2)\n",
    "        return offsets\n",
    "\n",
    "# Test model\n",
    "model = ErrorBarRegressor(backbone=cfg.backbone, pretrained=cfg.pretrained)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model: {cfg.backbone}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(2, 3, cfg.crop_height, cfg.crop_width).to(device)\n",
    "dummy_output = model(dummy_input)\n",
    "print(f\"Output shape: {dummy_output.shape}  # (batch_size, 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe0b9f0",
   "metadata": {},
   "source": [
    "## 5. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65db7bc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:06:35.777390Z",
     "iopub.status.busy": "2026-01-30T06:06:35.777050Z",
     "iopub.status.idle": "2026-01-30T06:06:46.140058Z",
     "shell.execute_reply": "2026-01-30T06:06:46.139300Z",
     "shell.execute_reply.started": "2026-01-30T06:06:35.777366Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total synthetic images: 2850\n",
      "Total real images: 150\n",
      "\n",
      "Data splits:\n",
      "  Synthetic train: 2707 images\n",
      "  Synthetic val:   143 images\n",
      "  Real train:      120 images (for fine-tuning)\n",
      "  Real test:       30 images (held-out)\n",
      "Dataset: 68961 data points from 2707 images\n",
      "Dataset: 3562 data points from 143 images\n",
      "\n",
      "Dataloaders ready:\n",
      "  Train: 2156 batches\n",
      "  Val:   112 batches\n"
     ]
    }
   ],
   "source": [
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Get all image IDs\n",
    "synthetic_ids = sorted([\n",
    "    os.path.splitext(f)[0]\n",
    "    for f in os.listdir(cfg.synthetic_images)\n",
    "    if f.endswith('.png')\n",
    "])\n",
    "\n",
    "real_ids = sorted([\n",
    "    os.path.splitext(f)[0]\n",
    "    for f in os.listdir(cfg.real_images)\n",
    "    if f.endswith('.png')\n",
    "])\n",
    "\n",
    "print(f\"Total synthetic images: {len(synthetic_ids)}\")\n",
    "print(f\"Total real images: {len(real_ids)}\")\n",
    "\n",
    "# Split synthetic into train/val\n",
    "random.shuffle(synthetic_ids)\n",
    "split_idx = int(len(synthetic_ids) * cfg.train_split)\n",
    "synth_train_ids = synthetic_ids[:split_idx]\n",
    "synth_val_ids = synthetic_ids[split_idx:]\n",
    "\n",
    "# Split real into train/test\n",
    "random.shuffle(real_ids)\n",
    "split_idx = int(len(real_ids) * cfg.real_train_split)\n",
    "real_train_ids = real_ids[:split_idx]\n",
    "real_test_ids = real_ids[split_idx:]\n",
    "\n",
    "print(f\"\\nData splits:\")\n",
    "print(f\"  Synthetic train: {len(synth_train_ids)} images\")\n",
    "print(f\"  Synthetic val:   {len(synth_val_ids)} images\")\n",
    "print(f\"  Real train:      {len(real_train_ids)} images (for fine-tuning)\")\n",
    "print(f\"  Real test:       {len(real_test_ids)} images (held-out)\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ErrorBarDataset(\n",
    "    synth_train_ids,\n",
    "    cfg.synthetic_images,\n",
    "    cfg.synthetic_labels,\n",
    "    cfg.crop_width,\n",
    "    cfg.crop_height,\n",
    "    cfg.max_offset,\n",
    "    train_transform,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "val_dataset = ErrorBarDataset(\n",
    "    synth_val_ids,\n",
    "    cfg.synthetic_images,\n",
    "    cfg.synthetic_labels,\n",
    "    cfg.crop_width,\n",
    "    cfg.crop_height,\n",
    "    cfg.max_offset,\n",
    "    val_transform\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataloaders ready:\")\n",
    "print(f\"  Train: {len(train_loader)} batches\")\n",
    "print(f\"  Val:   {len(val_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb14677",
   "metadata": {},
   "source": [
    "## 6. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f392a01f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:06:46.141379Z",
     "iopub.status.busy": "2026-01-30T06:06:46.141070Z",
     "iopub.status.idle": "2026-01-30T06:06:46.150523Z",
     "shell.execute_reply": "2026-01-30T06:06:46.149771Z",
     "shell.execute_reply.started": "2026-01-30T06:06:46.141356Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for images, targets in pbar:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device, max_offset):\n",
    "    \"\"\"Validate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    errors_up = []\n",
    "    errors_down = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(loader, desc='Validation'):\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Denormalize and compute errors\n",
    "            pred_up = outputs[:, 0].cpu().numpy() * max_offset\n",
    "            pred_down = outputs[:, 1].cpu().numpy() * max_offset\n",
    "            gt_up = targets[:, 0].cpu().numpy() * max_offset\n",
    "            gt_down = targets[:, 1].cpu().numpy() * max_offset\n",
    "            \n",
    "            errors_up.extend(np.abs(pred_up - gt_up))\n",
    "            errors_down.extend(np.abs(pred_down - gt_down))\n",
    "    \n",
    "    # Compute metrics\n",
    "    errors = np.array(errors_up + errors_down)\n",
    "    metrics = {\n",
    "        'loss': total_loss / len(loader),\n",
    "        'mean_error': float(errors.mean()),\n",
    "        'median_error': float(np.median(errors)),\n",
    "        'pct_within_2px': float((errors <= 2).mean()),\n",
    "        'pct_within_5px': float((errors <= 5).mean()),\n",
    "        'pct_within_10px': float((errors <= 10).mean())\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce15beb6",
   "metadata": {},
   "source": [
    "## 7. Train on Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1751464",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:06:46.151972Z",
     "iopub.status.busy": "2026-01-30T06:06:46.151488Z",
     "iopub.status.idle": "2026-01-30T06:52:55.878317Z",
     "shell.execute_reply": "2026-01-30T06:52:55.877507Z",
     "shell.execute_reply.started": "2026-01-30T06:06:46.151951Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on synthetic data...\n",
      "\n",
      "\n",
      "Epoch 1/6\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2156/2156 [07:20<00:00,  4.89it/s, loss=0.00153] \n",
      "Validation: 100%|██████████| 112/112 [00:21<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0024\n",
      "Val Loss:   0.0018\n",
      "Mean Error: 9.79px\n",
      "Median Error: 5.00px\n",
      "Within 2px:  20.8%\n",
      "Within 5px:  50.0%\n",
      "Within 10px: 71.4%\n",
      "✓ Saved best model\n",
      "\n",
      "Epoch 2/6\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2156/2156 [07:21<00:00,  4.88it/s, loss=6.41e-5] \n",
      "Validation: 100%|██████████| 112/112 [00:21<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0020\n",
      "Val Loss:   0.0015\n",
      "Mean Error: 8.62px\n",
      "Median Error: 4.71px\n",
      "Within 2px:  24.5%\n",
      "Within 5px:  51.2%\n",
      "Within 10px: 77.2%\n",
      "✓ Saved best model\n",
      "\n",
      "Epoch 3/6\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2156/2156 [07:18<00:00,  4.91it/s, loss=8.95e-5] \n",
      "Validation: 100%|██████████| 112/112 [00:21<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0018\n",
      "Val Loss:   0.0017\n",
      "Mean Error: 9.21px\n",
      "Median Error: 4.38px\n",
      "Within 2px:  23.2%\n",
      "Within 5px:  59.2%\n",
      "Within 10px: 74.0%\n",
      "\n",
      "Epoch 4/6\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2156/2156 [07:17<00:00,  4.92it/s, loss=0.000226]\n",
      "Validation: 100%|██████████| 112/112 [00:21<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0017\n",
      "Val Loss:   0.0014\n",
      "Mean Error: 8.34px\n",
      "Median Error: 4.72px\n",
      "Within 2px:  24.7%\n",
      "Within 5px:  50.8%\n",
      "Within 10px: 77.6%\n",
      "✓ Saved best model\n",
      "\n",
      "Epoch 5/6\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2156/2156 [07:17<00:00,  4.93it/s, loss=0.000349]\n",
      "Validation: 100%|██████████| 112/112 [00:21<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0016\n",
      "Val Loss:   0.0014\n",
      "Mean Error: 8.33px\n",
      "Median Error: 4.81px\n",
      "Within 2px:  25.3%\n",
      "Within 5px:  58.1%\n",
      "Within 10px: 77.6%\n",
      "✓ Saved best model\n",
      "\n",
      "Epoch 6/6\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2156/2156 [07:22<00:00,  4.88it/s, loss=0.00273] \n",
      "Validation: 100%|██████████| 112/112 [00:21<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0015\n",
      "Val Loss:   0.0013\n",
      "Mean Error: 8.13px\n",
      "Median Error: 4.73px\n",
      "Within 2px:  25.3%\n",
      "Within 5px:  58.8%\n",
      "Within 10px: 78.1%\n",
      "✓ Saved best model\n",
      "\n",
      "==================================================\n",
      "Training on synthetic data complete!\n",
      "Best validation loss: 0.0013\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = ErrorBarRegressor(backbone=cfg.backbone, pretrained=cfg.pretrained)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.SmoothL1Loss()  # Robust to outliers\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=cfg.learning_rate,\n",
    "    weight_decay=cfg.weight_decay\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=cfg.num_epochs_pretrain\n",
    ")\n",
    "\n",
    "print(\"Starting training on synthetic data...\\n\")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(cfg.num_epochs_pretrain):\n",
    "    print(f\"\\nEpoch {epoch+1}/{cfg.num_epochs_pretrain}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_metrics = validate(model, val_loader, criterion, device, cfg.max_offset)\n",
    "    val_losses.append(val_metrics['loss'])\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss:   {val_metrics['loss']:.4f}\")\n",
    "    print(f\"Mean Error: {val_metrics['mean_error']:.2f}px\")\n",
    "    print(f\"Median Error: {val_metrics['median_error']:.2f}px\")\n",
    "    print(f\"Within 2px:  {val_metrics['pct_within_2px']*100:.1f}%\")\n",
    "    print(f\"Within 5px:  {val_metrics['pct_within_5px']*100:.1f}%\")\n",
    "    print(f\"Within 10px: {val_metrics['pct_within_10px']*100:.1f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['loss'] < best_val_loss:\n",
    "        best_val_loss = val_metrics['loss']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_metrics': val_metrics\n",
    "        }, os.path.join(cfg.checkpoint_dir, 'best_synthetic.pth'))\n",
    "        print(\"✓ Saved best model\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training on synthetic data complete!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480487a",
   "metadata": {},
   "source": [
    "## 8. Fine-tune on Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa954e3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:52:55.879922Z",
     "iopub.status.busy": "2026-01-30T06:52:55.879665Z",
     "iopub.status.idle": "2026-01-30T06:58:58.677636Z",
     "shell.execute_reply": "2026-01-30T06:58:58.676812Z",
     "shell.execute_reply.started": "2026-01-30T06:52:55.879894Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best synthetic model\n",
      "Dataset: 3930 data points from 120 images\n",
      "Dataset: 909 data points from 30 images\n",
      "\n",
      "Starting fine-tuning on real data...\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:29<00:00,  4.21it/s, loss=0.0067] \n",
      "Validation: 100%|██████████| 29/29 [00:06<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0060\n",
      "Test Loss:  0.0067\n",
      "Mean Error: 18.03px\n",
      "Median Error: 8.71px\n",
      "Within 2px:  8.6%\n",
      "Within 5px:  20.4%\n",
      "Within 10px: 55.8%\n",
      "✓ Saved best fine-tuned model\n",
      "\n",
      "Epoch 2/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:29<00:00,  4.23it/s, loss=0.0144]  \n",
      "Validation: 100%|██████████| 29/29 [00:06<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0051\n",
      "Test Loss:  0.0063\n",
      "Mean Error: 16.91px\n",
      "Median Error: 9.34px\n",
      "Within 2px:  8.7%\n",
      "Within 5px:  24.9%\n",
      "Within 10px: 61.6%\n",
      "✓ Saved best fine-tuned model\n",
      "\n",
      "Epoch 3/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:29<00:00,  4.20it/s, loss=0.00158]\n",
      "Validation: 100%|██████████| 29/29 [00:06<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0047\n",
      "Test Loss:  0.0062\n",
      "Mean Error: 16.51px\n",
      "Median Error: 9.41px\n",
      "Within 2px:  9.2%\n",
      "Within 5px:  26.8%\n",
      "Within 10px: 63.3%\n",
      "✓ Saved best fine-tuned model\n",
      "\n",
      "Epoch 4/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:29<00:00,  4.20it/s, loss=0.00236] \n",
      "Validation: 100%|██████████| 29/29 [00:06<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0043\n",
      "Test Loss:  0.0061\n",
      "Mean Error: 16.14px\n",
      "Median Error: 9.44px\n",
      "Within 2px:  9.5%\n",
      "Within 5px:  28.8%\n",
      "Within 10px: 65.3%\n",
      "✓ Saved best fine-tuned model\n",
      "\n",
      "Epoch 5/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:29<00:00,  4.19it/s, loss=0.000909]\n",
      "Validation: 100%|██████████| 29/29 [00:06<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0042\n",
      "Test Loss:  0.0061\n",
      "Mean Error: 16.44px\n",
      "Median Error: 9.15px\n",
      "Within 2px:  9.3%\n",
      "Within 5px:  28.5%\n",
      "Within 10px: 64.3%\n",
      "\n",
      "Epoch 6/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:29<00:00,  4.24it/s, loss=0.000504]\n",
      "Validation: 100%|██████████| 29/29 [00:06<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0041\n",
      "Test Loss:  0.0061\n",
      "Mean Error: 16.16px\n",
      "Median Error: 9.04px\n",
      "Within 2px:  9.8%\n",
      "Within 5px:  29.6%\n",
      "Within 10px: 65.3%\n",
      "✓ Saved best fine-tuned model\n",
      "\n",
      "Epoch 7/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:29<00:00,  4.22it/s, loss=0.00126] \n",
      "Validation: 100%|██████████| 29/29 [00:06<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0039\n",
      "Test Loss:  0.0061\n",
      "Mean Error: 15.96px\n",
      "Median Error: 8.89px\n",
      "Within 2px:  10.1%\n",
      "Within 5px:  30.3%\n",
      "Within 10px: 66.6%\n",
      "\n",
      "Epoch 8/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:29<00:00,  4.23it/s, loss=0.00246] \n",
      "Validation: 100%|██████████| 29/29 [00:06<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0038\n",
      "Test Loss:  0.0061\n",
      "Mean Error: 15.92px\n",
      "Median Error: 8.83px\n",
      "Within 2px:  10.0%\n",
      "Within 5px:  30.6%\n",
      "Within 10px: 66.3%\n",
      "✓ Saved best fine-tuned model\n",
      "\n",
      "Epoch 9/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:28<00:00,  4.25it/s, loss=0.00521]\n",
      "Validation: 100%|██████████| 29/29 [00:06<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0039\n",
      "Test Loss:  0.0060\n",
      "Mean Error: 15.99px\n",
      "Median Error: 8.84px\n",
      "Within 2px:  9.8%\n",
      "Within 5px:  29.8%\n",
      "Within 10px: 66.1%\n",
      "✓ Saved best fine-tuned model\n",
      "\n",
      "Epoch 10/10\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 123/123 [00:28<00:00,  4.27it/s, loss=0.00304] \n",
      "Validation: 100%|██████████| 29/29 [00:06<00:00,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0039\n",
      "Test Loss:  0.0061\n",
      "Mean Error: 15.80px\n",
      "Median Error: 8.83px\n",
      "Within 2px:  10.3%\n",
      "Within 5px:  31.0%\n",
      "Within 10px: 66.9%\n",
      "\n",
      "==================================================\n",
      "Fine-tuning complete!\n",
      "Best test loss: 0.0060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load best synthetic model  \n",
    "checkpoint = torch.load(os.path.join(cfg.checkpoint_dir, 'best_synthetic.pth'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\"Loaded best synthetic model\")\n",
    "\n",
    "# Create real datasets\n",
    "real_train_dataset = ErrorBarDataset(\n",
    "    real_train_ids,\n",
    "    cfg.real_images,\n",
    "    cfg.real_labels,\n",
    "    cfg.crop_width,\n",
    "    cfg.crop_height,\n",
    "    cfg.max_offset,\n",
    "    train_transform,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "real_test_dataset = ErrorBarDataset(\n",
    "    real_test_ids,\n",
    "    cfg.real_images,\n",
    "    cfg.real_labels,\n",
    "    cfg.crop_width,\n",
    "    cfg.crop_height,\n",
    "    cfg.max_offset,\n",
    "    val_transform\n",
    ")\n",
    "\n",
    "real_train_loader = DataLoader(\n",
    "    real_train_dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "real_test_loader = DataLoader(\n",
    "    real_test_dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# New optimizer with lower learning rate for fine-tuning\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=cfg.learning_rate * 0.1,  # 10x lower LR\n",
    "    weight_decay=cfg.weight_decay\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=cfg.num_epochs_finetune\n",
    ")\n",
    "\n",
    "print(\"\\nStarting fine-tuning on real data...\\n\")\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(cfg.num_epochs_finetune):\n",
    "    print(f\"\\nEpoch {epoch+1}/{cfg.num_epochs_finetune}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train on real data\n",
    "    train_loss = train_epoch(model, real_train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Test on held-out real data\n",
    "    test_metrics = validate(model, real_test_loader, criterion, device, cfg.max_offset)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Test Loss:  {test_metrics['loss']:.4f}\")\n",
    "    print(f\"Mean Error: {test_metrics['mean_error']:.2f}px\")\n",
    "    print(f\"Median Error: {test_metrics['median_error']:.2f}px\")\n",
    "    print(f\"Within 2px:  {test_metrics['pct_within_2px']*100:.1f}%\")\n",
    "    print(f\"Within 5px:  {test_metrics['pct_within_5px']*100:.1f}%\")\n",
    "    print(f\"Within 10px: {test_metrics['pct_within_10px']*100:.1f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if test_metrics['loss'] < best_test_loss:\n",
    "        best_test_loss = test_metrics['loss']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'test_metrics': test_metrics\n",
    "        }, os.path.join(cfg.checkpoint_dir, 'best_finetuned.pth'))\n",
    "        print(\"✓ Saved best fine-tuned model\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Fine-tuning complete!\")\n",
    "print(f\"Best test loss: {best_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b899c8",
   "metadata": {},
   "source": [
    "## 9. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ca50103",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:58:58.679154Z",
     "iopub.status.busy": "2026-01-30T06:58:58.678897Z",
     "iopub.status.idle": "2026-01-30T06:59:05.165872Z",
     "shell.execute_reply": "2026-01-30T06:59:05.165020Z",
     "shell.execute_reply.started": "2026-01-30T06:58:58.679126Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL EVALUATION ON REAL TEST SET\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 29/29 [00:06<00:00,  4.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Performance:\n",
      "  Mean Error:    15.99px\n",
      "  Median Error:  8.84px\n",
      "\n",
      "Accuracy:\n",
      "  Within 2px:  9.8%\n",
      "  Within 5px:  29.8%\n",
      "  Within 10px: 66.1%\n",
      "\n",
      "Comparison to CV Baseline:\n",
      "  CV baseline (real): 28% @ 2px, 38-42% @ 5px, 60% @ 10px\n",
      "  ML model (real):    9.8% @ 2px, 29.8% @ 5px, 66.1% @ 10px\n",
      "\n",
      "  Improvement: +-10.2% @ 5px\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load best fine-tuned model\n",
    "checkpoint = torch.load(os.path.join(cfg.checkpoint_dir, 'best_finetuned.pth'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL EVALUATION ON REAL TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_metrics = validate(model, real_test_loader, criterion, device, cfg.max_offset)\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  Mean Error:    {test_metrics['mean_error']:.2f}px\")\n",
    "print(f\"  Median Error:  {test_metrics['median_error']:.2f}px\")\n",
    "print(f\"\\nAccuracy:\")\n",
    "print(f\"  Within 2px:  {test_metrics['pct_within_2px']*100:.1f}%\")\n",
    "print(f\"  Within 5px:  {test_metrics['pct_within_5px']*100:.1f}%\")\n",
    "print(f\"  Within 10px: {test_metrics['pct_within_10px']*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nComparison to CV Baseline:\")\n",
    "print(f\"  CV baseline (real): 28% @ 2px, 38-42% @ 5px, 60% @ 10px\")\n",
    "print(f\"  ML model (real):    {test_metrics['pct_within_2px']*100:.1f}% @ 2px, {test_metrics['pct_within_5px']*100:.1f}% @ 5px, {test_metrics['pct_within_10px']*100:.1f}% @ 10px\")\n",
    "print(f\"\\n  Improvement: +{(test_metrics['pct_within_5px']-0.40)*100:.1f}% @ 5px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d84fcc",
   "metadata": {},
   "source": [
    "## 10. Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60c57ca4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:59:05.167351Z",
     "iopub.status.busy": "2026-01-30T06:59:05.167090Z",
     "iopub.status.idle": "2026-01-30T06:59:05.179903Z",
     "shell.execute_reply": "2026-01-30T06:59:05.179379Z",
     "shell.execute_reply.started": "2026-01-30T06:59:05.167323Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference function defined\n"
     ]
    }
   ],
   "source": [
    "def detect_error_bars_ml(\n",
    "    image_path: str,\n",
    "    data_points: List[Dict[str, Any]],\n",
    "    model: nn.Module,\n",
    "    cfg: Config,\n",
    "    device: torch.device\n",
    ") -> Tuple[List[Dict[str, Any]], float]:\n",
    "    \"\"\"\n",
    "    Detect error bar endpoints using ML model.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image\n",
    "        data_points: List of {\"lineName\": str, \"points\": [{\"x\": float, \"y\": float}]}\n",
    "        model: Trained model\n",
    "        cfg: Configuration\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        error_bars: Detection results\n",
    "        avg_conf: Average confidence (dummy for now)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_w, img_h = img.size\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    error_bars = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for line in data_points:\n",
    "            line_name = line.get(\"lineName\", \"line\")\n",
    "            points_out = []\n",
    "            \n",
    "            for point in line.get(\"points\", []):\n",
    "                x, y = int(point[\"x\"]), int(point[\"y\"])\n",
    "                \n",
    "                # Extract crop\n",
    "                x1 = max(0, x - cfg.crop_width // 2)\n",
    "                x2 = min(img_w, x + cfg.crop_width // 2)\n",
    "                y1 = max(0, y - cfg.crop_height // 2)\n",
    "                y2 = min(img_h, y + cfg.crop_height // 2)\n",
    "                \n",
    "                crop = img.crop((x1, y1, x2, y2))\n",
    "                \n",
    "                # Pad if needed\n",
    "                if crop.size != (cfg.crop_width, cfg.crop_height):\n",
    "                    padded = Image.new('RGB', (cfg.crop_width, cfg.crop_height), (255, 255, 255))\n",
    "                    paste_x = (cfg.crop_width - crop.size[0]) // 2\n",
    "                    paste_y = (cfg.crop_height - crop.size[1]) // 2\n",
    "                    padded.paste(crop, (paste_x, paste_y))\n",
    "                    crop = padded\n",
    "                \n",
    "                # Transform and predict\n",
    "                crop_tensor = transform(crop).unsqueeze(0).to(device)\n",
    "                offsets = model(crop_tensor).cpu().numpy()[0]\n",
    "                \n",
    "                # Denormalize\n",
    "                dy_up = offsets[0] * cfg.max_offset\n",
    "                dy_down = offsets[1] * cfg.max_offset\n",
    "                \n",
    "                # Compute endpoints\n",
    "                y_up = int(round(y - dy_up))\n",
    "                y_down = int(round(y + dy_down))\n",
    "                \n",
    "                # Clamp to image bounds\n",
    "                y_up = max(0, min(img_h - 1, y_up))\n",
    "                y_down = max(0, min(img_h - 1, y_down))\n",
    "                \n",
    "                points_out.append({\n",
    "                    \"data_point\": {\"x\": float(point[\"x\"]), \"y\": float(point[\"y\"])},\n",
    "                    \"upper_error_bar\": {\"x\": float(point[\"x\"]), \"y\": float(y_up)},\n",
    "                    \"lower_error_bar\": {\"x\": float(point[\"x\"]), \"y\": float(y_down)},\n",
    "                    \"confidence\": 0.95  # Placeholder\n",
    "                })\n",
    "            \n",
    "            error_bars.append({\"lineName\": line_name, \"points\": points_out})\n",
    "    \n",
    "    return error_bars, 0.95\n",
    "\n",
    "print(\"Inference function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4dc519",
   "metadata": {},
   "source": [
    "## 11. Generate Output JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81b8b990",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T06:59:05.181200Z",
     "iopub.status.busy": "2026-01-30T06:59:05.180903Z",
     "iopub.status.idle": "2026-01-30T06:59:08.812193Z",
     "shell.execute_reply": "2026-01-30T06:59:08.811424Z",
     "shell.execute_reply.started": "2026-01-30T06:59:05.181168Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating outputs for 30 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:03<00:00,  8.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated 30 output files in outputs_ml_real\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_ml_outputs(\n",
    "    image_ids: List[str],\n",
    "    images_dir: str,\n",
    "    labels_dir: str,\n",
    "    output_dir: str,\n",
    "    model: nn.Module,\n",
    "    cfg: Config,\n",
    "    device: torch.device\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate output JSONs for all images using ML model.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Generating outputs for {len(image_ids)} images...\")\n",
    "    \n",
    "    for img_id in tqdm(image_ids):\n",
    "        img_path = os.path.join(images_dir, f\"{img_id}.png\")\n",
    "        label_path = os.path.join(labels_dir, f\"{img_id}.json\")\n",
    "        output_path = os.path.join(output_dir, f\"{img_id}.json\")\n",
    "        \n",
    "        if not os.path.exists(label_path):\n",
    "            continue\n",
    "        \n",
    "        # Load label\n",
    "        with open(label_path, 'r') as f:\n",
    "            label = json.load(f)\n",
    "        \n",
    "        # Convert to input format\n",
    "        data_points = []\n",
    "        for line in label:\n",
    "            line_name = line[\"label\"][\"lineName\"]\n",
    "            points = [\n",
    "                {\"x\": float(p[\"x\"]), \"y\": float(p[\"y\"])}\n",
    "                for p in line[\"points\"]\n",
    "                if p.get(\"label\", \"\") == \"\"\n",
    "            ]\n",
    "            if points:\n",
    "                data_points.append({\"lineName\": line_name, \"points\": points})\n",
    "        \n",
    "        # Run detection\n",
    "        error_bars, _ = detect_error_bars_ml(img_path, data_points, model, cfg, device)\n",
    "        \n",
    "        # Save output\n",
    "        output_data = {\n",
    "            \"image_file\": f\"{img_id}.png\",\n",
    "            \"error_bars\": error_bars\n",
    "        }\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(output_data, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Generated {len(image_ids)} output files in {output_dir}\")\n",
    "\n",
    "# Generate for real test set\n",
    "generate_ml_outputs(\n",
    "    real_test_ids,\n",
    "    cfg.real_images,\n",
    "    cfg.real_labels,\n",
    "    \"outputs_ml_real\",\n",
    "    model,\n",
    "    cfg,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d1d2874-096d-4071-8881-ff07a456fe7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T07:12:46.716173Z",
     "iopub.status.busy": "2026-01-30T07:12:46.715854Z",
     "iopub.status.idle": "2026-01-30T07:12:46.869699Z",
     "shell.execute_reply": "2026-01-30T07:12:46.868964Z",
     "shell.execute_reply.started": "2026-01-30T07:12:46.716142Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: kaggle/working/outputs_ml_real/ (stored 0%)\n",
      "  adding: kaggle/working/outputs_ml_real/18cf5500-bdf5-4e9e-8d35-dcedec20348e.json (deflated 91%)\n",
      "  adding: kaggle/working/outputs_ml_real/49a71e97-0215-42f8-b821-d3dcb04eeed7.json (deflated 92%)\n",
      "  adding: kaggle/working/outputs_ml_real/75bfe3c5-3e39-46c3-be78-c198482081a8.json (deflated 91%)\n",
      "  adding: kaggle/working/outputs_ml_real/0980c4a7-c946-42c8-92ba-7d9e5233d830.json (deflated 88%)\n",
      "  adding: kaggle/working/outputs_ml_real/e14de8ff-3fb6-4544-b736-0650ca6d81b9.json (deflated 89%)\n",
      "  adding: kaggle/working/outputs_ml_real/e4a2d0ba-fcbf-4cc4-980c-7f8bef626117.json (deflated 90%)\n",
      "  adding: kaggle/working/outputs_ml_real/07a084e8-7468-4540-b935-f51aa94ed15b.json (deflated 91%)\n",
      "  adding: kaggle/working/outputs_ml_real/d9b1d20d-f7c3-4b3a-b894-c70cb16c7f30.json (deflated 90%)\n",
      "  adding: kaggle/working/outputs_ml_real/06fc476e-9b3e-47ce-a23d-040918004aa0.json (deflated 88%)\n",
      "  adding: kaggle/working/outputs_ml_real/ba5a2550-b2bf-4dad-b80b-7583eeb109ab.json (deflated 84%)\n",
      "  adding: kaggle/working/outputs_ml_real/cd0ea86d-9155-4df2-aeed-5c94c759839d.json (deflated 92%)\n",
      "  adding: kaggle/working/outputs_ml_real/5a5e4525-4247-4e7b-aee0-baf4cdd39b01.json (deflated 90%)\n",
      "  adding: kaggle/working/outputs_ml_real/29ec902d-4d96-4120-b6fd-3f6a30d4c3e1.json (deflated 91%)\n",
      "  adding: kaggle/working/outputs_ml_real/1d662845-6904-4391-ae68-98d477f919b7.json (deflated 93%)\n",
      "  adding: kaggle/working/outputs_ml_real/cce3c2e0-753e-47a0-b6d8-028f08a19516.json (deflated 90%)\n",
      "  adding: kaggle/working/outputs_ml_real/b5f905dc-b2fd-4425-b324-2ecb696fcd08.json (deflated 92%)\n",
      "  adding: kaggle/working/outputs_ml_real/5e276726-85c5-46fb-89cd-8a80814274cc.json (deflated 90%)\n",
      "  adding: kaggle/working/outputs_ml_real/d4ddb2ba-8de4-444b-a896-8d49d59e1526.json (deflated 82%)\n",
      "  adding: kaggle/working/outputs_ml_real/9fd72934-e454-41a2-a72f-3de963f82b2b.json (deflated 81%)\n",
      "  adding: kaggle/working/outputs_ml_real/417b7b7a-af95-44db-9540-fbfbdac19a6b.json (deflated 87%)\n",
      "  adding: kaggle/working/outputs_ml_real/7741f494-0210-4744-8f8f-5ee9fae9edf4.json (deflated 91%)\n",
      "  adding: kaggle/working/outputs_ml_real/1af1b1fe-e6b1-420d-82c7-ffcc6e3c0170.json (deflated 88%)\n",
      "  adding: kaggle/working/outputs_ml_real/e886ed99-1aad-4efe-8119-1db7bf407aa0.json (deflated 88%)\n",
      "  adding: kaggle/working/outputs_ml_real/70a48da8-6cc2-4c02-8c4f-793860266446.json (deflated 86%)\n",
      "  adding: kaggle/working/outputs_ml_real/d74d1d64-b504-404e-85ec-e412624808cb.json (deflated 88%)\n",
      "  adding: kaggle/working/outputs_ml_real/df09c3ac-8e50-451a-834b-ca8ae048b9d7.json (deflated 88%)\n",
      "  adding: kaggle/working/outputs_ml_real/1be94e59-4faf-48a7-89db-b6cacf388ae5.json (deflated 89%)\n",
      "  adding: kaggle/working/outputs_ml_real/3041a99a-f851-44df-9114-422ffc016d01.json (deflated 90%)\n",
      "  adding: kaggle/working/outputs_ml_real/07cb7dea-7b12-42c9-abb3-1784fcf314f5.json (deflated 91%)\n",
      "  adding: kaggle/working/outputs_ml_real/2172dfce-7282-4d63-99bf-6a0ed7389318.json (deflated 91%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r \"ouputs_ml_real.zip\" \"/kaggle/working/outputs_ml_real\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562251c3-e0cf-4117-903b-a6f4fb4d7cf3",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9328694,
     "sourceId": 14604530,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9328860,
     "sourceId": 14604791,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
