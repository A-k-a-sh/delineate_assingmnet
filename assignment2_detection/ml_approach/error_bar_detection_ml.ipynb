{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14604530,"sourceType":"datasetVersion","datasetId":9328694},{"sourceId":14604791,"sourceType":"datasetVersion","datasetId":9328860}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"66ce085e","cell_type":"markdown","source":"## 1. Setup and Imports","metadata":{}},{"id":"26712eda","cell_type":"code","source":"import os\nimport json\nimport random\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Any\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\n# Set seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nprint(f\"PyTorch version: {torch.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T19:36:33.500862Z","iopub.execute_input":"2026-01-24T19:36:33.501372Z","iopub.status.idle":"2026-01-24T19:36:36.388698Z","shell.execute_reply.started":"2026-01-24T19:36:33.501346Z","shell.execute_reply":"2026-01-24T19:36:36.387873Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nPyTorch version: 2.8.0+cu126\n","output_type":"stream"}],"execution_count":1},{"id":"c0113a3b","cell_type":"markdown","source":"## 2. Configuration","metadata":{}},{"id":"4080fc3a","cell_type":"code","source":"@dataclass\nclass Config:\n    # Data paths\n    synthetic_images: str = \"/kaggle/input/med-synthetic-dataset/kaggle/working/dataset/images\"\n    synthetic_labels: str = \"/kaggle/input/med-synthetic-dataset/kaggle/working/dataset/labels\"\n    \n    real_images: str = \"/kaggle/input/med-real-dataset/dataset/images\"\n    real_labels: str = \"/kaggle/input/med-real-dataset/dataset/labels\"\n    \n    # Crop parameters\n    crop_width: int = 128      # Width of crop around data point\n    crop_height: int = 384     # Height of crop (more vertical space for error bars)\n    max_offset: float = 280.0  # Maximum expected error bar length (for normalization)\n    \n    # Model parameters\n    backbone: str = \"resnet18\"  # resnet18, resnet34\n    pretrained: bool = True\n    \n    # Training parameters\n    batch_size: int = 32\n    num_epochs_pretrain: int = 5\n    num_epochs_finetune: int = 20\n    learning_rate: float = 1e-3\n    weight_decay: float = 1e-4\n    \n    # Data split\n    train_split: float = 0.95   # 95% of synthetic for training\n    real_train_split: float = 0.8  # 80% of real for fine-tuning\n    \n    # Output\n    checkpoint_dir: str = \"checkpoints\"\n    results_dir: str = \"results\"\n\ncfg = Config()\nos.makedirs(cfg.checkpoint_dir, exist_ok=True)\nos.makedirs(cfg.results_dir, exist_ok=True)\n\nprint(f\"Configuration:\")\nprint(f\"  Crop size: {cfg.crop_width}×{cfg.crop_height}\")\nprint(f\"  Backbone: {cfg.backbone}\")\nprint(f\"  Batch size: {cfg.batch_size}\")\nprint(f\"  Device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T19:36:36.390394Z","iopub.execute_input":"2026-01-24T19:36:36.390728Z","iopub.status.idle":"2026-01-24T19:36:36.399713Z","shell.execute_reply.started":"2026-01-24T19:36:36.390703Z","shell.execute_reply":"2026-01-24T19:36:36.398689Z"}},"outputs":[{"name":"stdout","text":"Configuration:\n  Crop size: 128×384\n  Backbone: resnet18\n  Batch size: 32\n  Device: cuda\n","output_type":"stream"}],"execution_count":2},{"id":"dd087a86","cell_type":"markdown","source":"## 3. Dataset Class","metadata":{}},{"id":"65d389ca","cell_type":"code","source":"class ErrorBarDataset(Dataset):\n    \"\"\"\n    Dataset for error bar detection.\n    \n    Returns crops around each data point with ground truth offsets.\n    \"\"\"\n    \n    def __init__(\n        self,\n        image_ids: List[str],\n        images_dir: str,\n        labels_dir: str,\n        crop_width: int = 128,\n        crop_height: int = 384,\n        max_offset: float = 280.0,\n        transform=None,\n        augment: bool = False\n    ):\n        self.image_ids = image_ids\n        self.images_dir = images_dir\n        self.labels_dir = labels_dir\n        self.crop_width = crop_width\n        self.crop_height = crop_height\n        self.max_offset = max_offset\n        self.transform = transform\n        self.augment = augment\n        \n        # Build index of all data points\n        self.samples = []\n        self._build_index()\n        \n        print(f\"Dataset: {len(self.samples)} data points from {len(image_ids)} images\")\n    \n    def _build_index(self):\n        \"\"\"Build index of all data points across all images.\"\"\"\n        for img_id in self.image_ids:\n            label_path = os.path.join(self.labels_dir, f\"{img_id}.json\")\n            if not os.path.exists(label_path):\n                continue\n            \n            with open(label_path, 'r') as f:\n                label = json.load(f)\n            \n            # Extract all actual data points (not anchor points)\n            for line in label:\n                for point in line[\"points\"]:\n                    if point.get(\"label\", \"\") == \"\":  # actual data point\n                        self.samples.append({\n                            \"image_id\": img_id,\n                            \"x\": float(point[\"x\"]),\n                            \"y\": float(point[\"y\"]),\n                            \"dy_up\": float(point.get(\"topBarPixelDistance\", 0)),\n                            \"dy_down\": float(point.get(\"bottomBarPixelDistance\", 0))\n                        })\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        \n        # Load image\n        img_path = os.path.join(self.images_dir, f\"{sample['image_id']}.png\")\n        img = Image.open(img_path).convert('RGB')\n        img_w, img_h = img.size\n        \n        # Extract crop around data point\n        x, y = int(sample['x']), int(sample['y'])\n        \n        # Crop boundaries (centered on data point)\n        x1 = max(0, x - self.crop_width // 2)\n        x2 = min(img_w, x + self.crop_width // 2)\n        y1 = max(0, y - self.crop_height // 2)\n        y2 = min(img_h, y + self.crop_height // 2)\n        \n        crop = img.crop((x1, y1, x2, y2))\n        \n        # Pad if crop is too small (near edges)\n        if crop.size != (self.crop_width, self.crop_height):\n            padded = Image.new('RGB', (self.crop_width, self.crop_height), (255, 255, 255))\n            paste_x = (self.crop_width - crop.size[0]) // 2\n            paste_y = (self.crop_height - crop.size[1]) // 2\n            padded.paste(crop, (paste_x, paste_y))\n            crop = padded\n        \n        # Apply transforms\n        if self.transform:\n            crop = self.transform(crop)\n        \n        # Ground truth offsets (normalized)\n        dy_up = sample['dy_up'] / self.max_offset\n        dy_down = sample['dy_down'] / self.max_offset\n        target = torch.tensor([dy_up, dy_down], dtype=torch.float32)\n        \n        return crop, target\n\nprint(\"Dataset class defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T19:36:36.400965Z","iopub.execute_input":"2026-01-24T19:36:36.401252Z","iopub.status.idle":"2026-01-24T19:36:36.419978Z","shell.execute_reply.started":"2026-01-24T19:36:36.401216Z","shell.execute_reply":"2026-01-24T19:36:36.419152Z"}},"outputs":[{"name":"stdout","text":"Dataset class defined\n","output_type":"stream"}],"execution_count":3},{"id":"135d1899","cell_type":"markdown","source":"## 4. Model Architecture","metadata":{}},{"id":"c5220812","cell_type":"code","source":"class ErrorBarRegressor(nn.Module):\n    \"\"\"\n    CNN-based regressor for error bar endpoint detection.\n    \n    Architecture:\n    - ResNet backbone (pretrained on ImageNet)\n    - Global average pooling\n    - FC layers → 2 outputs (dy_up, dy_down)\n    \"\"\"\n    \n    def __init__(self, backbone='resnet18', pretrained=True):\n        super().__init__()\n        \n        # Load pretrained backbone\n        if backbone == 'resnet18':\n            resnet = models.resnet18(pretrained=pretrained)\n            feat_dim = 512\n        elif backbone == 'resnet34':\n            resnet = models.resnet34(pretrained=pretrained)\n            feat_dim = 512\n        else:\n            raise ValueError(f\"Unknown backbone: {backbone}\")\n        \n        # Remove final FC layer\n        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n        \n        # Regression head\n        self.regressor = nn.Sequential(\n            nn.Linear(feat_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, 2)  # (dy_up, dy_down)\n        )\n    \n    def forward(self, x):\n        # Extract features\n        features = self.backbone(x)  # (batch, feat_dim, 1, 1)\n        features = features.view(features.size(0), -1)  # (batch, feat_dim)\n        \n        # Regress offsets\n        offsets = self.regressor(features)  # (batch, 2)\n        return offsets\n\n# Test model\nmodel = ErrorBarRegressor(backbone=cfg.backbone, pretrained=cfg.pretrained)\nmodel = model.to(device)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Model: {cfg.backbone}\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")\n\n# Test forward pass\ndummy_input = torch.randn(2, 3, cfg.crop_height, cfg.crop_width).to(device)\ndummy_output = model(dummy_input)\nprint(f\"Output shape: {dummy_output.shape}  # (batch_size, 2)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T19:36:36.421629Z","iopub.execute_input":"2026-01-24T19:36:36.421885Z","iopub.status.idle":"2026-01-24T19:36:37.081812Z","shell.execute_reply.started":"2026-01-24T19:36:36.421864Z","shell.execute_reply":"2026-01-24T19:36:37.081022Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Model: resnet18\nTotal parameters: 11,324,418\nTrainable parameters: 11,324,418\nOutput shape: torch.Size([2, 2])  # (batch_size, 2)\n","output_type":"stream"}],"execution_count":4},{"id":"9fe0b9f0","cell_type":"markdown","source":"## 5. Data Loading","metadata":{}},{"id":"65db7bc6","cell_type":"code","source":"# Transforms\ntrain_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                       std=[0.229, 0.224, 0.225])  # ImageNet stats\n])\n\nval_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                       std=[0.229, 0.224, 0.225])\n])\n\n# Get all image IDs\nsynthetic_ids = sorted([\n    os.path.splitext(f)[0]\n    for f in os.listdir(cfg.synthetic_images)\n    if f.endswith('.png')\n])\n\nreal_ids = sorted([\n    os.path.splitext(f)[0]\n    for f in os.listdir(cfg.real_images)\n    if f.endswith('.png')\n])\n\nprint(f\"Total synthetic images: {len(synthetic_ids)}\")\nprint(f\"Total real images: {len(real_ids)}\")\n\n# Split synthetic into train/val\nrandom.shuffle(synthetic_ids)\nsplit_idx = int(len(synthetic_ids) * cfg.train_split)\nsynth_train_ids = synthetic_ids[:split_idx]\nsynth_val_ids = synthetic_ids[split_idx:]\n\n# Split real into train/test\nrandom.shuffle(real_ids)\nsplit_idx = int(len(real_ids) * cfg.real_train_split)\nreal_train_ids = real_ids[:split_idx]\nreal_test_ids = real_ids[split_idx:]\n\nprint(f\"\\nData splits:\")\nprint(f\"  Synthetic train: {len(synth_train_ids)} images\")\nprint(f\"  Synthetic val:   {len(synth_val_ids)} images\")\nprint(f\"  Real train:      {len(real_train_ids)} images (for fine-tuning)\")\nprint(f\"  Real test:       {len(real_test_ids)} images (held-out)\")\n\n# Create datasets\ntrain_dataset = ErrorBarDataset(\n    synth_train_ids,\n    cfg.synthetic_images,\n    cfg.synthetic_labels,\n    cfg.crop_width,\n    cfg.crop_height,\n    cfg.max_offset,\n    train_transform,\n    augment=True\n)\n\nval_dataset = ErrorBarDataset(\n    synth_val_ids,\n    cfg.synthetic_images,\n    cfg.synthetic_labels,\n    cfg.crop_width,\n    cfg.crop_height,\n    cfg.max_offset,\n    val_transform\n)\n\n# Create dataloaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=cfg.batch_size,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=cfg.batch_size,\n    shuffle=False,\n    num_workers=4,\n    pin_memory=True\n)\n\nprint(f\"\\nDataloaders ready:\")\nprint(f\"  Train: {len(train_loader)} batches\")\nprint(f\"  Val:   {len(val_loader)} batches\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T19:36:37.082879Z","iopub.execute_input":"2026-01-24T19:36:37.083308Z","iopub.status.idle":"2026-01-24T19:36:40.087982Z","shell.execute_reply.started":"2026-01-24T19:36:37.083270Z","shell.execute_reply":"2026-01-24T19:36:40.087129Z"}},"outputs":[{"name":"stdout","text":"Total synthetic images: 2850\nTotal real images: 150\n\nData splits:\n  Synthetic train: 2707 images\n  Synthetic val:   143 images\n  Real train:      120 images (for fine-tuning)\n  Real test:       30 images (held-out)\nDataset: 68961 data points from 2707 images\nDataset: 3562 data points from 143 images\n\nDataloaders ready:\n  Train: 2156 batches\n  Val:   112 batches\n","output_type":"stream"}],"execution_count":5},{"id":"feb14677","cell_type":"markdown","source":"## 6. Training Functions","metadata":{}},{"id":"f392a01f","cell_type":"code","source":"def train_epoch(model, loader, criterion, optimizer, device):\n    \"\"\"Train for one epoch.\"\"\"\n    model.train()\n    total_loss = 0\n    \n    pbar = tqdm(loader, desc='Training')\n    for images, targets in pbar:\n        images = images.to(device)\n        targets = targets.to(device)\n        \n        # Forward\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, targets)\n        \n        # Backward\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        pbar.set_postfix({'loss': loss.item()})\n    \n    return total_loss / len(loader)\n\n\ndef validate(model, loader, criterion, device, max_offset):\n    \"\"\"Validate model.\"\"\"\n    model.eval()\n    total_loss = 0\n    errors_up = []\n    errors_down = []\n    \n    with torch.no_grad():\n        for images, targets in tqdm(loader, desc='Validation'):\n            images = images.to(device)\n            targets = targets.to(device)\n            \n            outputs = model(images)\n            loss = criterion(outputs, targets)\n            total_loss += loss.item()\n            \n            # Denormalize and compute errors\n            pred_up = outputs[:, 0].cpu().numpy() * max_offset\n            pred_down = outputs[:, 1].cpu().numpy() * max_offset\n            gt_up = targets[:, 0].cpu().numpy() * max_offset\n            gt_down = targets[:, 1].cpu().numpy() * max_offset\n            \n            errors_up.extend(np.abs(pred_up - gt_up))\n            errors_down.extend(np.abs(pred_down - gt_down))\n    \n    # Compute metrics\n    errors = np.array(errors_up + errors_down)\n    metrics = {\n        'loss': total_loss / len(loader),\n        'mean_error': float(errors.mean()),\n        'median_error': float(np.median(errors)),\n        'pct_within_2px': float((errors <= 2).mean()),\n        'pct_within_5px': float((errors <= 5).mean()),\n        'pct_within_10px': float((errors <= 10).mean())\n    }\n    \n    return metrics\n\nprint(\"Training functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T19:36:40.089154Z","iopub.execute_input":"2026-01-24T19:36:40.089804Z","iopub.status.idle":"2026-01-24T19:36:40.098873Z","shell.execute_reply.started":"2026-01-24T19:36:40.089779Z","shell.execute_reply":"2026-01-24T19:36:40.098182Z"}},"outputs":[{"name":"stdout","text":"Training functions defined\n","output_type":"stream"}],"execution_count":6},{"id":"ce15beb6","cell_type":"markdown","source":"## 7. Train on Synthetic Data","metadata":{}},{"id":"e1751464","cell_type":"code","source":"# Initialize model\nmodel = ErrorBarRegressor(backbone=cfg.backbone, pretrained=cfg.pretrained)\nmodel = model.to(device)\n\n# Loss and optimizer\ncriterion = nn.SmoothL1Loss()  # Robust to outliers\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=cfg.learning_rate,\n    weight_decay=cfg.weight_decay\n)\n\n# Learning rate scheduler\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer,\n    T_max=cfg.num_epochs_pretrain\n)\n\nprint(\"Starting training on synthetic data...\\n\")\n\nbest_val_loss = float('inf')\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(cfg.num_epochs_pretrain):\n    print(f\"\\nEpoch {epoch+1}/{cfg.num_epochs_pretrain}\")\n    print(\"-\" * 50)\n    \n    # Train\n    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n    train_losses.append(train_loss)\n    \n    # Validate\n    val_metrics = validate(model, val_loader, criterion, device, cfg.max_offset)\n    val_losses.append(val_metrics['loss'])\n    \n    # Update learning rate\n    scheduler.step()\n    \n    # Print metrics\n    print(f\"Train Loss: {train_loss:.4f}\")\n    print(f\"Val Loss:   {val_metrics['loss']:.4f}\")\n    print(f\"Mean Error: {val_metrics['mean_error']:.2f}px\")\n    print(f\"Median Error: {val_metrics['median_error']:.2f}px\")\n    print(f\"Within 2px:  {val_metrics['pct_within_2px']*100:.1f}%\")\n    print(f\"Within 5px:  {val_metrics['pct_within_5px']*100:.1f}%\")\n    print(f\"Within 10px: {val_metrics['pct_within_10px']*100:.1f}%\")\n    \n    # Save best model\n    if val_metrics['loss'] < best_val_loss:\n        best_val_loss = val_metrics['loss']\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_metrics': val_metrics\n        }, os.path.join(cfg.checkpoint_dir, 'best_synthetic.pth'))\n        print(\"✓ Saved best model\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Training on synthetic data complete!\")\nprint(f\"Best validation loss: {best_val_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T19:36:40.099874Z","iopub.execute_input":"2026-01-24T19:36:40.100166Z","iopub.status.idle":"2026-01-24T20:16:03.459366Z","shell.execute_reply.started":"2026-01-24T19:36:40.100144Z","shell.execute_reply":"2026-01-24T20:16:03.458443Z"}},"outputs":[{"name":"stdout","text":"Starting training on synthetic data...\n\n\nEpoch 1/5\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2156/2156 [07:30<00:00,  4.78it/s, loss=0.00186] \nValidation: 100%|██████████| 112/112 [00:22<00:00,  5.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0024\nVal Loss:   0.0024\nMean Error: 12.83px\nMedian Error: 7.37px\nWithin 2px:  16.8%\nWithin 5px:  39.1%\nWithin 10px: 57.9%\n✓ Saved best model\n\nEpoch 2/5\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2156/2156 [07:29<00:00,  4.80it/s, loss=0.000142]\nValidation: 100%|██████████| 112/112 [00:21<00:00,  5.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0020\nVal Loss:   0.0015\nMean Error: 8.65px\nMedian Error: 4.42px\nWithin 2px:  23.2%\nWithin 5px:  59.2%\nWithin 10px: 75.4%\n✓ Saved best model\n\nEpoch 3/5\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2156/2156 [07:29<00:00,  4.79it/s, loss=4.47e-6] \nValidation: 100%|██████████| 112/112 [00:22<00:00,  5.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0018\nVal Loss:   0.0014\nMean Error: 8.48px\nMedian Error: 4.76px\nWithin 2px:  25.1%\nWithin 5px:  57.8%\nWithin 10px: 77.3%\n✓ Saved best model\n\nEpoch 4/5\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2156/2156 [07:29<00:00,  4.79it/s, loss=0.000225]\nValidation: 100%|██████████| 112/112 [00:21<00:00,  5.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0017\nVal Loss:   0.0013\nMean Error: 8.21px\nMedian Error: 4.95px\nWithin 2px:  24.5%\nWithin 5px:  50.1%\nWithin 10px: 77.6%\n✓ Saved best model\n\nEpoch 5/5\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2156/2156 [07:32<00:00,  4.77it/s, loss=0.000192]\nValidation: 100%|██████████| 112/112 [00:22<00:00,  5.04it/s]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0015\nVal Loss:   0.0013\nMean Error: 8.08px\nMedian Error: 4.74px\nWithin 2px:  24.9%\nWithin 5px:  58.9%\nWithin 10px: 78.7%\n\n==================================================\nTraining on synthetic data complete!\nBest validation loss: 0.0013\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"id":"1480487a","cell_type":"markdown","source":"## 8. Fine-tune on Real Data","metadata":{}},{"id":"aa954e3d","cell_type":"code","source":"# Load best synthetic model  \ncheckpoint = torch.load(os.path.join(cfg.checkpoint_dir, 'best_synthetic.pth'))\nmodel.load_state_dict(checkpoint['model_state_dict'])\nprint(\"Loaded best synthetic model\")\n\n# Create real datasets\nreal_train_dataset = ErrorBarDataset(\n    real_train_ids,\n    cfg.real_images,\n    cfg.real_labels,\n    cfg.crop_width,\n    cfg.crop_height,\n    cfg.max_offset,\n    train_transform,\n    augment=True\n)\n\nreal_test_dataset = ErrorBarDataset(\n    real_test_ids,\n    cfg.real_images,\n    cfg.real_labels,\n    cfg.crop_width,\n    cfg.crop_height,\n    cfg.max_offset,\n    val_transform\n)\n\nreal_train_loader = DataLoader(\n    real_train_dataset,\n    batch_size=cfg.batch_size,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True\n)\n\nreal_test_loader = DataLoader(\n    real_test_dataset,\n    batch_size=cfg.batch_size,\n    shuffle=False,\n    num_workers=4,\n    pin_memory=True\n)\n\n# New optimizer with lower learning rate for fine-tuning\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=cfg.learning_rate * 0.1,  # 10x lower LR\n    weight_decay=cfg.weight_decay\n)\n\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer,\n    T_max=cfg.num_epochs_finetune\n)\n\nprint(\"\\nStarting fine-tuning on real data...\\n\")\n\nbest_test_loss = float('inf')\n\nfor epoch in range(cfg.num_epochs_finetune):\n    print(f\"\\nEpoch {epoch+1}/{cfg.num_epochs_finetune}\")\n    print(\"-\" * 50)\n    \n    # Train on real data\n    train_loss = train_epoch(model, real_train_loader, criterion, optimizer, device)\n    \n    # Test on held-out real data\n    test_metrics = validate(model, real_test_loader, criterion, device, cfg.max_offset)\n    \n    scheduler.step()\n    \n    # Print metrics\n    print(f\"Train Loss: {train_loss:.4f}\")\n    print(f\"Test Loss:  {test_metrics['loss']:.4f}\")\n    print(f\"Mean Error: {test_metrics['mean_error']:.2f}px\")\n    print(f\"Median Error: {test_metrics['median_error']:.2f}px\")\n    print(f\"Within 2px:  {test_metrics['pct_within_2px']*100:.1f}%\")\n    print(f\"Within 5px:  {test_metrics['pct_within_5px']*100:.1f}%\")\n    print(f\"Within 10px: {test_metrics['pct_within_10px']*100:.1f}%\")\n    \n    # Save best model\n    if test_metrics['loss'] < best_test_loss:\n        best_test_loss = test_metrics['loss']\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'test_metrics': test_metrics\n        }, os.path.join(cfg.checkpoint_dir, 'best_finetuned.pth'))\n        print(\"✓ Saved best fine-tuned model\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Fine-tuning complete!\")\nprint(f\"Best test loss: {best_test_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T20:16:03.460748Z","iopub.execute_input":"2026-01-24T20:16:03.461046Z","iopub.status.idle":"2026-01-24T20:28:26.330758Z","shell.execute_reply.started":"2026-01-24T20:16:03.461015Z","shell.execute_reply":"2026-01-24T20:28:26.329911Z"}},"outputs":[{"name":"stdout","text":"Loaded best synthetic model\nDataset: 3930 data points from 120 images\nDataset: 909 data points from 30 images\n\nStarting fine-tuning on real data...\n\n\nEpoch 1/20\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 123/123 [00:30<00:00,  4.08it/s, loss=0.00219]\nValidation: 100%|██████████| 29/29 [00:06<00:00,  4.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0058\nTest Loss:  0.0068\nMean Error: 17.93px\nMedian Error: 8.82px\nWithin 2px:  9.7%\nWithin 5px:  23.5%\nWithin 10px: 59.1%\n✓ Saved best fine-tuned model\n\nEpoch 2/20\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 123/123 [00:29<00:00,  4.12it/s, loss=0.00662]\nValidation: 100%|██████████| 29/29 [00:06<00:00,  4.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0050\nTest Loss:  0.0067\nMean Error: 17.93px\nMedian Error: 9.55px\nWithin 2px:  8.3%\nWithin 5px:  23.4%\nWithin 10px: 59.5%\n✓ Saved best fine-tuned model\n\nEpoch 3/20\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 123/123 [00:29<00:00,  4.12it/s, loss=0.0141]  \nValidation: 100%|██████████| 29/29 [00:06<00:00,  4.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0047\nTest Loss:  0.0068\nMean Error: 18.33px\nMedian Error: 9.16px\nWithin 2px:  8.3%\nWithin 5px:  24.7%\nWithin 10px: 58.4%\n\nEpoch 4/20\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 123/123 [00:30<00:00,  4.10it/s, loss=0.00214] \nValidation: 100%|██████████| 29/29 [00:06<00:00,  4.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0044\nTest Loss:  0.0068\nMean Error: 18.01px\nMedian Error: 9.40px\nWithin 2px:  8.5%\nWithin 5px:  25.3%\nWithin 10px: 59.2%\n\nEpoch 5/20\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 123/123 [00:30<00:00,  4.10it/s, loss=0.00245] \nValidation: 100%|██████████| 29/29 [00:07<00:00,  4.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0042\nTest Loss:  0.0067\nMean Error: 17.39px\nMedian Error: 9.41px\nWithin 2px:  8.9%\nWithin 5px:  26.4%\nWithin 10px: 62.7%\n\nEpoch 6/20\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 123/123 [00:30<00:00,  4.10it/s, loss=0.00137]\nValidation: 100%|██████████| 29/29 [00:06<00:00,  4.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0041\nTest Loss:  0.0066\nMean Error: 17.10px\nMedian Error: 9.00px\nWithin 2px:  10.0%\nWithin 5px:  27.2%\nWithin 10px: 62.2%\n✓ Saved best fine-tuned model\n\nEpoch 7/20\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 123/123 [00:29<00:00,  4.13it/s, loss=0.000738]\nValidation: 100%|██████████| 29/29 [00:06<00:00,  4.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0040\nTest Loss:  0.0064\nMean Error: 16.58px\nMedian Error: 8.76px\nWithin 2px:  11.8%\nWithin 5px:  28.8%\nWithin 10px: 64.5%\n✓ Saved best fine-tuned model\n\nEpoch 8/20\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 123/123 [00:30<00:00,  4.10it/s, loss=0.00109] \nValidation: 100%|██████████| 29/29 [00:06<00:00,  4.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0039\nTest Loss:  0.0069\nMean Error: 16.99px\nMedian Error: 8.43px\nWithin 2px:  11.8%\nWithin 5px:  30.4%\nWithin 10px: 67.5%\n\nEpoch 9/20\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 123/123 [00:29<00:00,  4.11it/s, loss=0.00224] \nValidation: 100%|██████████| 29/29 [00:06<00:00,  4.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0037\nTest Loss:  0.0063\nMean Error: 16.44px\nMedian Error: 8.61px\nWithin 2px:  11.1%\nWithin 5px:  29.5%\nWithin 10px: 64.7%\n✓ Saved best fine-tuned model\n\nEpoch 10/20\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 123/123 [00:30<00:00,  4.09it/s, loss=0.00551] \nValidation: 100%|██████████| 29/29 [00:06<00:00,  4.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0037\nTest Loss:  0.0064\nMean Error: 16.69px\nMedian Error: 9.04px\nWithin 2px:  10.1%\nWithin 5px:  28.6%\nWithin 10px: 63.2%\n\nEpoch 11/20\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 123/123 [00:29<00:00,  4.12it/s, loss=0.00302] \nValidation: 100%|██████████| 29/29 [00:07<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0035\nTest Loss:  0.0065\nMean Error: 16.38px\nMedian Error: 8.38px\nWithin 2px:  12.2%\nWithin 5px:  31.4%\nWithin 10px: 66.3%\n\nEpoch 12/20\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 123/123 [00:30<00:00,  4.08it/s, loss=0.00536] \nValidation: 100%|██████████| 29/29 [00:06<00:00,  4.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0034\nTest Loss:  0.0064\nMean Error: 16.48px\nMedian Error: 8.67px\nWithin 2px:  11.2%\nWithin 5px:  29.6%\nWithin 10px: 64.8%\n\nEpoch 13/20\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 123/123 [00:30<00:00,  4.01it/s, loss=0.0012]  \nValidation: 100%|██████████| 29/29 [00:07<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0034\nTest Loss:  0.0063\nMean Error: 16.56px\nMedian Error: 8.91px\nWithin 2px:  10.8%\nWithin 5px:  28.5%\nWithin 10px: 64.0%\n\nEpoch 14/20\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 123/123 [00:30<00:00,  4.08it/s, loss=0.00203] \nValidation: 100%|██████████| 29/29 [00:07<00:00,  4.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0033\nTest Loss:  0.0063\nMean Error: 16.49px\nMedian Error: 8.77px\nWithin 2px:  11.1%\nWithin 5px:  27.9%\nWithin 10px: 63.8%\n✓ Saved best fine-tuned model\n\nEpoch 15/20\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 123/123 [00:30<00:00,  4.09it/s, loss=0.00358] \nValidation: 100%|██████████| 29/29 [00:06<00:00,  4.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0033\nTest Loss:  0.0064\nMean Error: 16.28px\nMedian Error: 8.19px\nWithin 2px:  12.0%\nWithin 5px:  30.3%\nWithin 10px: 66.0%\n\nEpoch 16/20\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 123/123 [00:30<00:00,  4.10it/s, loss=0.00247] \nValidation: 100%|██████████| 29/29 [00:06<00:00,  4.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0032\nTest Loss:  0.0064\nMean Error: 16.74px\nMedian Error: 8.83px\nWithin 2px:  11.0%\nWithin 5px:  28.3%\nWithin 10px: 63.8%\n\nEpoch 17/20\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 123/123 [00:30<00:00,  4.07it/s, loss=0.000859]\nValidation: 100%|██████████| 29/29 [00:06<00:00,  4.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0032\nTest Loss:  0.0064\nMean Error: 16.57px\nMedian Error: 8.81px\nWithin 2px:  11.1%\nWithin 5px:  28.3%\nWithin 10px: 64.6%\n\nEpoch 18/20\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 123/123 [00:30<00:00,  4.02it/s, loss=0.00135] \nValidation: 100%|██████████| 29/29 [00:06<00:00,  4.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0034\nTest Loss:  0.0063\nMean Error: 16.28px\nMedian Error: 8.20px\nWithin 2px:  11.8%\nWithin 5px:  30.6%\nWithin 10px: 66.2%\n\nEpoch 19/20\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 123/123 [00:30<00:00,  4.09it/s, loss=0.00166] \nValidation: 100%|██████████| 29/29 [00:07<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0032\nTest Loss:  0.0064\nMean Error: 16.59px\nMedian Error: 8.84px\nWithin 2px:  11.7%\nWithin 5px:  28.3%\nWithin 10px: 63.6%\n\nEpoch 20/20\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 123/123 [00:30<00:00,  4.07it/s, loss=0.00174] \nValidation: 100%|██████████| 29/29 [00:06<00:00,  4.23it/s]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0032\nTest Loss:  0.0063\nMean Error: 16.38px\nMedian Error: 8.44px\nWithin 2px:  11.8%\nWithin 5px:  29.5%\nWithin 10px: 65.1%\n\n==================================================\nFine-tuning complete!\nBest test loss: 0.0063\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"id":"79b899c8","cell_type":"markdown","source":"## 9. Final Evaluation","metadata":{}},{"id":"1ca50103","cell_type":"code","source":"# Load best fine-tuned model\ncheckpoint = torch.load(os.path.join(cfg.checkpoint_dir, 'best_finetuned.pth'))\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\nprint(\"=\"*60)\nprint(\"FINAL EVALUATION ON REAL TEST SET\")\nprint(\"=\"*60)\n\ntest_metrics = validate(model, real_test_loader, criterion, device, cfg.max_offset)\n\nprint(f\"\\nTest Set Performance:\")\nprint(f\"  Mean Error:    {test_metrics['mean_error']:.2f}px\")\nprint(f\"  Median Error:  {test_metrics['median_error']:.2f}px\")\nprint(f\"\\nAccuracy:\")\nprint(f\"  Within 2px:  {test_metrics['pct_within_2px']*100:.1f}%\")\nprint(f\"  Within 5px:  {test_metrics['pct_within_5px']*100:.1f}%\")\nprint(f\"  Within 10px: {test_metrics['pct_within_10px']*100:.1f}%\")\n\nprint(f\"\\nComparison to CV Baseline:\")\nprint(f\"  CV baseline (real): 28% @ 2px, 38-42% @ 5px, 60% @ 10px\")\nprint(f\"  ML model (real):    {test_metrics['pct_within_2px']*100:.1f}% @ 2px, {test_metrics['pct_within_5px']*100:.1f}% @ 5px, {test_metrics['pct_within_10px']*100:.1f}% @ 10px\")\nprint(f\"\\n  Improvement: +{(test_metrics['pct_within_5px']-0.40)*100:.1f}% @ 5px\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T20:28:26.332151Z","iopub.execute_input":"2026-01-24T20:28:26.332427Z","iopub.status.idle":"2026-01-24T20:28:33.111354Z","shell.execute_reply.started":"2026-01-24T20:28:26.332396Z","shell.execute_reply":"2026-01-24T20:28:33.110553Z"}},"outputs":[{"name":"stdout","text":"============================================================\nFINAL EVALUATION ON REAL TEST SET\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 29/29 [00:06<00:00,  4.39it/s]","output_type":"stream"},{"name":"stdout","text":"\nTest Set Performance:\n  Mean Error:    16.49px\n  Median Error:  8.77px\n\nAccuracy:\n  Within 2px:  11.1%\n  Within 5px:  27.9%\n  Within 10px: 63.8%\n\nComparison to CV Baseline:\n  CV baseline (real): 28% @ 2px, 38-42% @ 5px, 60% @ 10px\n  ML model (real):    11.1% @ 2px, 27.9% @ 5px, 63.8% @ 10px\n\n  Improvement: +-12.1% @ 5px\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"id":"c5d84fcc","cell_type":"markdown","source":"## 10. Inference Pipeline","metadata":{}},{"id":"60c57ca4","cell_type":"code","source":"def detect_error_bars_ml(\n    image_path: str,\n    data_points: List[Dict[str, Any]],\n    model: nn.Module,\n    cfg: Config,\n    device: torch.device\n) -> Tuple[List[Dict[str, Any]], float]:\n    \"\"\"\n    Detect error bar endpoints using ML model.\n    \n    Args:\n        image_path: Path to image\n        data_points: List of {\"lineName\": str, \"points\": [{\"x\": float, \"y\": float}]}\n        model: Trained model\n        cfg: Configuration\n        device: torch device\n    \n    Returns:\n        error_bars: Detection results\n        avg_conf: Average confidence (dummy for now)\n    \"\"\"\n    model.eval()\n    \n    # Load image\n    img = Image.open(image_path).convert('RGB')\n    img_w, img_h = img.size\n    \n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    error_bars = []\n    \n    with torch.no_grad():\n        for line in data_points:\n            line_name = line.get(\"lineName\", \"line\")\n            points_out = []\n            \n            for point in line.get(\"points\", []):\n                x, y = int(point[\"x\"]), int(point[\"y\"])\n                \n                # Extract crop\n                x1 = max(0, x - cfg.crop_width // 2)\n                x2 = min(img_w, x + cfg.crop_width // 2)\n                y1 = max(0, y - cfg.crop_height // 2)\n                y2 = min(img_h, y + cfg.crop_height // 2)\n                \n                crop = img.crop((x1, y1, x2, y2))\n                \n                # Pad if needed\n                if crop.size != (cfg.crop_width, cfg.crop_height):\n                    padded = Image.new('RGB', (cfg.crop_width, cfg.crop_height), (255, 255, 255))\n                    paste_x = (cfg.crop_width - crop.size[0]) // 2\n                    paste_y = (cfg.crop_height - crop.size[1]) // 2\n                    padded.paste(crop, (paste_x, paste_y))\n                    crop = padded\n                \n                # Transform and predict\n                crop_tensor = transform(crop).unsqueeze(0).to(device)\n                offsets = model(crop_tensor).cpu().numpy()[0]\n                \n                # Denormalize\n                dy_up = offsets[0] * cfg.max_offset\n                dy_down = offsets[1] * cfg.max_offset\n                \n                # Compute endpoints\n                y_up = int(round(y - dy_up))\n                y_down = int(round(y + dy_down))\n                \n                # Clamp to image bounds\n                y_up = max(0, min(img_h - 1, y_up))\n                y_down = max(0, min(img_h - 1, y_down))\n                \n                points_out.append({\n                    \"data_point\": {\"x\": float(point[\"x\"]), \"y\": float(point[\"y\"])},\n                    \"upper_error_bar\": {\"x\": float(point[\"x\"]), \"y\": float(y_up)},\n                    \"lower_error_bar\": {\"x\": float(point[\"x\"]), \"y\": float(y_down)},\n                    \"confidence\": 0.95  # Placeholder\n                })\n            \n            error_bars.append({\"lineName\": line_name, \"points\": points_out})\n    \n    return error_bars, 0.95\n\nprint(\"Inference function defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T20:28:33.113858Z","iopub.execute_input":"2026-01-24T20:28:33.114200Z","iopub.status.idle":"2026-01-24T20:28:33.125852Z","shell.execute_reply.started":"2026-01-24T20:28:33.114171Z","shell.execute_reply":"2026-01-24T20:28:33.124907Z"}},"outputs":[{"name":"stdout","text":"Inference function defined\n","output_type":"stream"}],"execution_count":10},{"id":"6c4dc519","cell_type":"markdown","source":"## 11. Generate Output JSONs","metadata":{}},{"id":"81b8b990","cell_type":"code","source":"def generate_ml_outputs(\n    image_ids: List[str],\n    images_dir: str,\n    labels_dir: str,\n    output_dir: str,\n    model: nn.Module,\n    cfg: Config,\n    device: torch.device\n):\n    \"\"\"\n    Generate output JSONs for all images using ML model.\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    print(f\"Generating outputs for {len(image_ids)} images...\")\n    \n    for img_id in tqdm(image_ids):\n        img_path = os.path.join(images_dir, f\"{img_id}.png\")\n        label_path = os.path.join(labels_dir, f\"{img_id}.json\")\n        output_path = os.path.join(output_dir, f\"{img_id}.json\")\n        \n        if not os.path.exists(label_path):\n            continue\n        \n        # Load label\n        with open(label_path, 'r') as f:\n            label = json.load(f)\n        \n        # Convert to input format\n        data_points = []\n        for line in label:\n            line_name = line[\"label\"][\"lineName\"]\n            points = [\n                {\"x\": float(p[\"x\"]), \"y\": float(p[\"y\"])}\n                for p in line[\"points\"]\n                if p.get(\"label\", \"\") == \"\"\n            ]\n            if points:\n                data_points.append({\"lineName\": line_name, \"points\": points})\n        \n        # Run detection\n        error_bars, _ = detect_error_bars_ml(img_path, data_points, model, cfg, device)\n        \n        # Save output\n        output_data = {\n            \"image_file\": f\"{img_id}.png\",\n            \"error_bars\": error_bars\n        }\n        \n        with open(output_path, 'w') as f:\n            json.dump(output_data, f, indent=2)\n    \n    print(f\"✓ Generated {len(image_ids)} output files in {output_dir}\")\n\n# Generate for real test set\ngenerate_ml_outputs(\n    real_test_ids,\n    cfg.real_images,\n    cfg.real_labels,\n    \"outputs_ml_real\",\n    model,\n    cfg,\n    device\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T20:28:33.126583Z","iopub.execute_input":"2026-01-24T20:28:33.126773Z","iopub.status.idle":"2026-01-24T20:28:36.799553Z","shell.execute_reply.started":"2026-01-24T20:28:33.126756Z","shell.execute_reply":"2026-01-24T20:28:36.798846Z"}},"outputs":[{"name":"stdout","text":"Generating outputs for 30 images...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 30/30 [00:03<00:00,  8.21it/s]","output_type":"stream"},{"name":"stdout","text":"✓ Generated 30 output files in outputs_ml_real\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"id":"7d1d2874-096d-4071-8881-ff07a456fe7a","cell_type":"code","source":"!zip \"ouputs_ml_real.zip\" \"/kaggle/working/outputs_ml_real\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T20:41:15.381913Z","iopub.execute_input":"2026-01-24T20:41:15.382294Z","iopub.status.idle":"2026-01-24T20:41:15.545753Z","shell.execute_reply.started":"2026-01-24T20:41:15.382265Z","shell.execute_reply":"2026-01-24T20:41:15.545085Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/outputs_ml_real/ (stored 0%)\n","output_type":"stream"}],"execution_count":12},{"id":"562251c3-e0cf-4117-903b-a6f4fb4d7cf3","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}